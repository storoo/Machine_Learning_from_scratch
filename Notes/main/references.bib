@online{baevskiData2vecGeneralFramework2022a,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  shorttitle = {Data2vec},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  date = {2022-10-25},
  eprint = {2202.03555},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.03555},
  url = {http://arxiv.org/abs/2202.03555},
  urldate = {2025-04-22},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\37SLKS8K\\Baevski et al. - 2022 - data2vec A General Framework for Self-supervised Learning in Speech, Vision and Language.pdf;C\:\\Users\\storo\\Zotero\\storage\\S4VRKAU8\\2202.html}
}

@online{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  eprint = {1409.0473},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2025-04-20},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\EBEXUMGP\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf;C\:\\Users\\storo\\Zotero\\storage\\F3YIPAVC\\1409.html}
}

@article{bonnabelStochasticGradientDescent2013,
  title = {Stochastic {{Gradient Descent}} on {{Riemannian Manifolds}}},
  author = {Bonnabel, Silvère},
  date = {2013-09},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {58},
  number = {9},
  pages = {2217--2229},
  issn = {1558-2523},
  doi = {10.1109/TAC.2013.2254619},
  url = {https://ieeexplore.ieee.org/document/6487381},
  urldate = {2025-04-29},
  abstract = {Stochastic gradient descent is a simple approach to find the local minima of a cost function whose evaluations are corrupted by noise. In this paper, we develop a procedure extending stochastic gradient descent algorithms to the case where the function is defined on a Riemannian manifold. We prove that, as in the Euclidian case, the gradient descent algorithm converges to a critical point of the cost function. The algorithm has numerous potential applications, and is illustrated here by four examples. In particular a novel gossip algorithm on the set of covariance matrices is derived and tested numerically.},
  keywords = {Approximation methods,Convergence,Cost function,Covariance matrices,Manifolds,Nonlinear identification,Riemannian geometry,Standards,stochastic approximation,Trajectory},
  file = {C\:\\Users\\storo\\Zotero\\storage\\GE3I8EEA\\Bonnabel - 2013 - Stochastic Gradient Descent on Riemannian Manifolds.pdf;C\:\\Users\\storo\\Zotero\\storage\\DK9MCVJ5\\6487381.html}
}

@online{collobertNaturalLanguageProcessing2011,
  title = {Natural {{Language Processing}} (Almost) from {{Scratch}}},
  author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  date = {2011-03-02},
  eprint = {1103.0398},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1103.0398},
  url = {http://arxiv.org/abs/1103.0398},
  urldate = {2025-04-29},
  abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\KDTDKP6R\\Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf;C\:\\Users\\storo\\Zotero\\storage\\HGIWDVL5\\1103.html}
}

@online{desaiHyperbolicImageTextRepresentations2024,
  title = {Hyperbolic {{Image-Text Representations}}},
  author = {Desai, Karan and Nickel, Maximilian and Rajpurohit, Tanmay and Johnson, Justin and Vedantam, Ramakrishna},
  date = {2024-01-18},
  eprint = {2304.09172},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.09172},
  url = {http://arxiv.org/abs/2304.09172},
  urldate = {2025-04-22},
  abstract = {Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept "dog" entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text datasets. Our results show that MERU learns a highly interpretable and structured representation space while being competitive with CLIP's performance on standard multi-modal tasks like image classification and image-text retrieval. Our code and models are available at https://www.github.com/facebookresearch/meru},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\SRJB9U5W\\Desai et al. - 2024 - Hyperbolic Image-Text Representations.pdf;C\:\\Users\\storo\\Zotero\\storage\\MSFLYMPR\\2304.html}
}

@online{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2025-04-29},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\storo\\Zotero\\storage\\VFQGV3Y4\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf;C\:\\Users\\storo\\Zotero\\storage\\IPNBY3WE\\1810.html}
}

@online{dhingraEmbeddingTextHyperbolic2018,
  title = {Embedding {{Text}} in {{Hyperbolic Spaces}}},
  author = {Dhingra, Bhuwan and Shallue, Christopher J. and Norouzi, Mohammad and Dai, Andrew M. and Dahl, George E.},
  date = {2018-06-12},
  eprint = {1806.04313},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.04313},
  url = {http://arxiv.org/abs/1806.04313},
  urldate = {2025-04-29},
  abstract = {Natural language text exhibits hierarchical structure in a variety of respects. Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data. Recent work by Nickel \& Kiela (2017) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs. In this work, we extend their method with a re-parameterization technique that allows us to learn hyperbolic embeddings of arbitrarily parameterized objects. We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora. The resulting embeddings seem to encode certain intuitive notions of hierarchy, such as word-context frequency and phrase constituency. However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model's learned hierarchies more difficult than for models that learn explicit edges between items. The learned hyperbolic embeddings show improvements over Euclidean embeddings in some -- but not all -- downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\DJ3QWBQ7\\Dhingra et al. - 2018 - Embedding Text in Hyperbolic Spaces.pdf;C\:\\Users\\storo\\Zotero\\storage\\277HE5RR\\1806.html}
}

@inproceedings{evainLeBenchmarkReproducibleFramework2021,
  title = {{{LeBenchmark}}: {{A Reproducible Framework}} for {{Assessing Self-Supervised Representation Learning}} from {{Speech}}},
  shorttitle = {{{LeBenchmark}}},
  booktitle = {Interspeech 2021},
  author = {Evain, Solene and Nguyen, Ha and Le, Hang and Boito, Marcely Zanon and Mdhaffar, Salima and Alisamir, Sina and Tong, Ziyi and Tomashenko, Natalia and Dinarelli, Marco and Parcollet, Titouan and Allauzen, Alexandre and Esteve, Yannick and Lecouteux, Benjamin and Portet, Francois and Rossato, Solange and Ringeval, Fabien and Schwab, Didier and Besacier, Laurent},
  date = {2021-08-30},
  eprint = {2104.11462},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1439--1443},
  doi = {10.21437/Interspeech.2021-556},
  url = {http://arxiv.org/abs/2104.11462},
  urldate = {2025-04-22},
  abstract = {Self-Supervised Learning (SSL) using huge unlabeled data has been successfully explored for image and natural language processing. Recent works also investigated SSL from speech. They were notably successful to improve performance on downstream tasks such as automatic speech recognition (ASR). While these works suggest it is possible to reduce dependence on labeled data for building efficient speech systems, their evaluation was mostly made on ASR and using multiple and heterogeneous experimental settings (most of them for English). This questions the objective comparison of SSL approaches and the evaluation of their impact on building speech systems. In this paper, we propose LeBenchmark: a reproducible framework for assessing SSL from speech. It not only includes ASR (high and low resource) tasks but also spoken language understanding, speech translation and emotion recognition. We also focus on speech technologies in a language different than English: French. SSL models of different sizes are trained from carefully sourced and documented datasets. Experiments show that SSL is beneficial for most but not all tasks which confirms the need for exhaustive and reliable benchmarks to evaluate its real impact. LeBenchmark is shared with the scientific community for reproducible research in SSL from speech.},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\storo\\Zotero\\storage\\KEJNWXU3\\Evain et al. - 2021 - LeBenchmark A Reproducible Framework for Assessing Self-Supervised Representation Learning from Spe.pdf;C\:\\Users\\storo\\Zotero\\storage\\275TLBBL\\2104.html}
}

@online{fein-ashleyHVTComprehensiveVision2024,
  title = {{{HVT}}: {{A Comprehensive Vision Framework}} for {{Learning}} in {{Non-Euclidean Space}}},
  shorttitle = {{{HVT}}},
  author = {Fein-Ashley, Jacob and Feng, Ethan and Pham, Minh},
  date = {2024-09-26},
  eprint = {2409.16897},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.16897},
  url = {http://arxiv.org/abs/2409.16897},
  urldate = {2025-04-29},
  abstract = {Data representation in non-Euclidean spaces has proven effective for capturing hierarchical and complex relationships in real-world datasets. Hyperbolic spaces, in particular, provide efficient embeddings for hierarchical structures. This paper introduces the Hyperbolic Vision Transformer (HVT), a novel extension of the Vision Transformer (ViT) that integrates hyperbolic geometry. While traditional ViTs operate in Euclidean space, our method enhances the self-attention mechanism by leveraging hyperbolic distance and M\textbackslash "obius transformations. This enables more effective modeling of hierarchical and relational dependencies in image data. We present rigorous mathematical formulations, showing how hyperbolic geometry can be incorporated into attention layers, feed-forward networks, and optimization. We offer improved performance for image classification using the ImageNet dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\storo\Zotero\storage\ZZ9RIHI8\Fein-Ashley et al. - 2024 - HVT A Comprehensive Vision Framework for Learning in Non-Euclidean Space.pdf}
}

@online{leFlauBERTUnsupervisedLanguage2020,
  title = {{{FlauBERT}}: {{Unsupervised Language Model Pre-training}} for {{French}}},
  shorttitle = {{{FlauBERT}}},
  author = {Le, Hang and Vial, Loïc and Frej, Jibril and Segonne, Vincent and Coavoux, Maximin and Lecouteux, Benjamin and Allauzen, Alexandre and Crabbé, Benoît and Besacier, Laurent and Schwab, Didier},
  date = {2020-03-12},
  eprint = {1912.05372},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1912.05372},
  url = {http://arxiv.org/abs/1912.05372},
  urldate = {2025-04-22},
  abstract = {Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\XU7UGSCI\\Le et al. - 2020 - FlauBERT Unsupervised Language Model Pre-training for French.pdf;C\:\\Users\\storo\\Zotero\\storage\\A76PIHTV\\1912.html}
}

@online{mettesHyperbolicDeepLearning2023,
  title = {Hyperbolic {{Deep Learning}} in {{Computer Vision}}: {{A Survey}}},
  shorttitle = {Hyperbolic {{Deep Learning}} in {{Computer Vision}}},
  author = {Mettes, Pascal and Atigh, Mina Ghadimi and Keller-Ressel, Martin and Gu, Jeffrey and Yeung, Serena},
  date = {2023-05-11},
  eprint = {2305.06611},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.06611},
  url = {http://arxiv.org/abs/2305.06611},
  urldate = {2025-04-29},
  abstract = {Deep representation learning is a ubiquitous part of modern computer vision. While Euclidean space has been the de facto standard manifold for learning visual representations, hyperbolic space has recently gained rapid traction for learning in computer vision. Specifically, hyperbolic learning has shown a strong potential to embed hierarchical structures, learn from limited samples, quantify uncertainty, add robustness, limit error severity, and more. In this paper, we provide a categorization and in-depth overview of current literature on hyperbolic learning for computer vision. We research both supervised and unsupervised literature and identify three main research themes in each direction. We outline how hyperbolic learning is performed in all themes and discuss the main research problems that benefit from current advances in hyperbolic learning for computer vision. Moreover, we provide a high-level intuition behind hyperbolic geometry and outline open research questions to further advance research in this direction.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\storo\\Zotero\\storage\\XVWLWWGG\\Mettes et al. - 2023 - Hyperbolic Deep Learning in Computer Vision A Survey.pdf;C\:\\Users\\storo\\Zotero\\storage\\A8MD5XVC\\2305.html}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  date = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  urldate = {2025-05-21},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
  file = {C:\Users\storo\Zotero\storage\234IWHS9\Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf}
}

@online{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-01-16},
  url = {https://arxiv.org/abs/1301.3781v3},
  urldate = {2025-05-21},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  langid = {english},
  organization = {arXiv.org},
  file = {C:\Users\storo\Zotero\storage\KTWX2NPT\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf}
}

@online{moreiraHyperbolicVsEuclidean2023,
  title = {Hyperbolic vs {{Euclidean Embeddings}} in {{Few-Shot Learning}}: {{Two Sides}} of the {{Same Coin}}},
  shorttitle = {Hyperbolic vs {{Euclidean Embeddings}} in {{Few-Shot Learning}}},
  author = {Moreira, Gabriel and Marques, Manuel and Costeira, João Paulo and Hauptmann, Alexander},
  date = {2023-09-18},
  eprint = {2309.10013},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.10013},
  url = {http://arxiv.org/abs/2309.10013},
  urldate = {2025-04-29},
  abstract = {Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\textbackslash 'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\4P2GBAIN\\Moreira et al. - 2023 - Hyperbolic vs Euclidean Embeddings in Few-Shot Learning Two Sides of the Same Coin.pdf;C\:\\Users\\storo\\Zotero\\storage\\K6S7AI3Y\\2309.html}
}

@inproceedings{nickelLearningContinuousHierarchies2018,
  title = {Learning {{Continuous Hierarchies}} in the {{Lorentz Model}} of {{Hyperbolic Geometry}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Nickel, Maximillian and Kiela, Douwe},
  date = {2018-07-03},
  pages = {3779--3788},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/nickel18a.html},
  urldate = {2025-05-06},
  abstract = {We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincar\{é\}-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincar\{é\} embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company’s organizational structure as well as reveal historical relationships between language families.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C:\Users\storo\Zotero\storage\5TYLUX8S\Nickel and Kiela - 2018 - Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry.pdf}
}

@online{nickelPoincareEmbeddingsLearning2017,
  title = {Poincaré {{Embeddings}} for {{Learning Hierarchical Representations}}},
  author = {Nickel, Maximilian and Kiela, Douwe},
  date = {2017-05-26},
  eprint = {1705.08039},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.08039},
  url = {http://arxiv.org/abs/1705.08039},
  urldate = {2025-04-22},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\textbackslash 'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\textbackslash 'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\TUCFTB7A\\Nickel and Kiela - 2017 - Poincaré Embeddings for Learning Hierarchical Representations.pdf;C\:\\Users\\storo\\Zotero\\storage\\A5NHXIBY\\1705.html}
}

@online{pantagruelprojectteamConstructionDevaluationGrands,
  title = {Construction et d'évaluation de grands modèles de langue multimodaux et inclusifs (écrit, oral, pictogrammes) pour le français général et clinique},
  author = {{Pantagruel Project Team}},
  url = {https://anr.fr/Projet-ANR-23-IAS1-0001},
  urldate = {2025-04-29},
  abstract = {Le projet Pantagruel est une initiative ambitieuse qui vise à développer et à évaluer des modèles linguistiques multimodaux (écrit, oral, pictogrammes) et inclusifs pour le français. Le projet s'appuie sur l'expertise de chercheurs de différentes disciplines, notamment l'informatique, le traitement du signal, la sociologie et la linguistique, afin de garantir la diversité des perspectives ainsi que la fiabilité et la pertinence des résultats. Les principales contributions du projet sont le développement de modèles autosupervisés librement accessibles pour le français, comprenant une à trois des modalités pour les domaines généraux et cliniques. Le projet ne se contentera pas de produire des modèles mais il concevra également des bancs d’essais permettant d'évaluer la généralisation de ce type de modèles en s'appuyant sur l’expérience gagnée lors des projets FlauBERT et LeBenchmark. Une part du projet sera consacrée aux biais et stéréotypes véhiculés dans les corpus d'entraînement et dans les modèles en aval. Une réflexion sera menée avec un comité éthique, pour limiter un effet amplificateur de biais au sein des corpus d'entraînement, en particulier en travaillant sur les caractéristiques démographiques des locuteurices (pour l'oral audio ou retranscrit) et des auteurices (pour une partie des données écrites). Nous pourrons ainsi comparer les modèles appris sur des corpus d'entraînement aux proportions variables pour ces caractéristiques, comme par exemple le genre. Cette étude permettra de quantifier dans quelle mesure les prédictions des modèles de langue sont des reflets fiables des corpus en amont, et de mieux contrôler la façon par laquelle ils peuvent être utilisés comme outils de recherche pour les sciences sociales. Le projet développera des composants logiciels qui faciliteront l'intégration des modèles de langage dans diverses applications et permettront le développement de solutions innovantes exploitant la puissance des modèles de langues du français multimodaux. Ces outils sont en particulier destinés aux non-informaticiens tels que ceux membres du consortium (sociologues, linguistes, médecins,  orthophonistes), des chercheurs d'autres domaines ou des artistes. Le projet Pantagruel a ainsi le potentiel de faire progresser de manière significative l'état de l'art en matière de modèles de langues multimodaux et d'avoir un impact positif sur un large éventail de domaines appliqués, des soins de santé aux arts en passant par les sciences humaines et sociales.},
  langid = {french},
  organization = {Agence nationale de la recherche},
  file = {C:\Users\storo\Zotero\storage\83XLY6Y7\Projet-ANR-23-IAS1-0001.html}
}

@inproceedings{radfordLanguageModelsAre2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeff and Child, R. and Luan, D. and Amodei, Dario and Sutskever, I.},
  date = {2019},
  url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  urldate = {2025-05-07},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.}
}

@online{saRepresentationTradeoffsHyperbolic2018,
  title = {Representation {{Tradeoffs}} for {{Hyperbolic Embeddings}}},
  author = {Sa, Christopher De and Gu, Albert and Ré, Christopher and Sala, Frederic},
  date = {2018-04-24},
  eprint = {1804.03329},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1804.03329},
  url = {http://arxiv.org/abs/1804.03329},
  urldate = {2025-04-29},
  abstract = {Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\UPUWHBEP\\Sa et al. - 2018 - Representation Tradeoffs for Hyperbolic Embeddings.pdf;C\:\\Users\\storo\\Zotero\\storage\\U5K63WQ5\\1804.html}
}

@inproceedings{segonneJargonSuiteModeles2024,
  title = {Jargon : Une suite de modèles de langues et de référentiels d'évaluation pour les domaines spécialisés du français},
  shorttitle = {Jargon},
  author = {Segonne, Vincent and Mannion, Aidan and Alonzo-Canul, Laura and Audibert, Alexandre and Liu, Xingyu and Macaire, Cécile and Pupier, Adrien and Zhou, Yongxin and Aguiar, Mathilde and Herron, Felix and Norré, Magali and Amini, Massih-Reza and Bouillon, Pierrette and Eshkol-Taravela, Iris and Esparança-Rodier, Emmanuelle and François, Thomas and Goeuriot, Lorraine and Goulian, Jérôme and Lafourcade, Mathieu and Lecouteux, Benjamin and Portet, François and Ringeval, Fabien and Vandeghinste, Vincent and Coavoux, Maximin and Dinarelli, Marco and Schwab, Didier},
  date = {2024-07-08},
  volume = {2 : traductions d'articles publiés},
  pages = {9},
  publisher = {ATALA \& AFPC},
  url = {https://inria.hal.science/hal-04622997},
  urldate = {2025-04-29},
  abstract = {Les modèles de langue préentraînés (PLM) constituent aujourd’hui de facto l’épine dorsale de la plupart des systèmes de traitement automatique des langues. Dans cet article, nous présentons Jargon, une famille de PLMs pour des domaines spécialisés du français, en nous focalisant sur trois domaines : la parole transcrite, le domaine clinique / biomédical, et le domaine juridique. Nous utilisons une architecture de transformeur basée sur des méthodes computationnellement efficaces(LinFormer) puisque ces domaines impliquent souvent le traitement de longs documents. Nous évaluons et comparons nos modèles à des modèles de l’état de l’art sur un ensemble varié de tâches et de corpus d’évaluation, dont certains sont introduits dans notre article. Nous rassemblons les jeux de données dans un nouveau référentiel d’évaluation en langue française pour ces trois domaines. Nous comparons également diverses configurations d’entraînement : préentraînement prolongé en apprentissage autosupervisé sur les données spécialisées, préentraînement à partir de zéro, ainsi que préentraînement mono et multi-domaines. Nos expérimentations approfondies dans des domaines spécialisés montrent qu’il est possible d’atteindre des performances compétitives en aval, même lors d’un préentraînement avec le mécanisme d’attention approximatif de LinFormer. Pour une reproductibilité totale, nous publions les modèles et les données de préentraînement, ainsi que les corpus utilisés.},
  eventtitle = {31ème Conférence sur le Traitement Automatique des Langues Naturelles (TALN 2024)},
  langid = {french},
  file = {C:\Users\storo\Zotero\storage\PS8MMYNK\Segonne et al. - 2024 - Jargon  Une suite de modèles de langues et de référentiels d'évaluation pour les domaines spécialis.pdf}
}

@online{sinhaLearningStructuredRepresentations2024,
  title = {Learning {{Structured Representations}} with {{Hyperbolic Embeddings}}},
  author = {Sinha, Aditya and Zeng, Siqi and Yamada, Makoto and Zhao, Han},
  date = {2024-12-02},
  eprint = {2412.01023},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.01023},
  url = {http://arxiv.org/abs/2412.01023},
  urldate = {2025-04-29},
  abstract = {Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013]. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss, and can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios. For a better understanding of structured representation, we perform eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at \textbackslash url\{https://github.com/uiuctml/HypStructure\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\329NJHZD\\Sinha et al. - 2024 - Learning Structured Representations with Hyperbolic Embeddings.pdf;C\:\\Users\\storo\\Zotero\\storage\\CDKFB33U\\2412.html}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014-12-08},
  series = {{{NIPS}}'14},
  volume = {2},
  pages = {3104--3112},
  publisher = {MIT Press},
  location = {Cambridge, MA, USA},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.}
}

@article{tenenbaumGlobalGeometricFramework2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and family=Silva, given=Vin, prefix=de, useprefix=false and Langford, John C.},
  date = {2000-12-22},
  journaltitle = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.290.5500.2319},
  url = {https://www.science.org/doi/10.1126/science.290.5500.2319},
  urldate = {2025-04-22},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and family=Kaiser, given=Ł, prefix=ukasz, useprefix=false and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2025-04-19},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\storo\Zotero\storage\NB28868L\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{wilsonSphericalHyperbolicEmbeddings2014,
  title = {Spherical and {{Hyperbolic Embeddings}} of {{Data}}},
  author = {Wilson, Richard C. and Hancock, Edwin R. and Pekalska, Elżbieta and Duin, Robert P.W.},
  date = {2014-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {11},
  pages = {2255--2269},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2014.2316836},
  url = {https://ieeexplore.ieee.org/abstract/document/6787114},
  urldate = {2025-04-29},
  abstract = {Many computer vision and pattern recognition problems may be posed as the analysis of a set of dissimilarities between objects. For many types of data, these dissimilarities are not euclidean (i.e., they do not represent the distances between points in a euclidean space), and therefore cannot be isometrically embedded in a euclidean space. Examples include shape-dissimilarities, graph distances and mesh geodesic distances. In this paper, we provide a means of embedding such non-euclidean data onto surfaces of constant curvature. We aim to embed the data on a space whose radius of curvature is determined by the dissimilarity data. The space can be either of positive curvature (spherical) or of negative curvature (hyperbolic). We give an efficient method for solving the spherical and hyperbolic embedding problems on symmetric dissimilarity data. Our approach gives the radius of curvature and a method for approximating the objects as points on a hyperspherical manifold without optimisation. For objects which do not reside exactly on the manifold, we develop a optimisation-based procedure for approximate embedding on a hyperspherical manifold. We use the exponential map between the manifold and its local tangent space to solve the optimisation problem locally in the euclidean tangent space. This process is efficient enough to allow us to embed data sets of several thousand objects. We apply our method to a variety of data including time warping functions, shape similarities, graph similarity and gesture similarity data. In each case the embedding maintains the local structure of the data while placing the points in a metric space.},
  keywords = {Eigenvalues and eigenfunctions,Embedding,Geometry,hyperbolic,Kernel,Manifolds,Measurement,non-euclidean,Optimization,spherical,Vectors},
  file = {C\:\\Users\\storo\\Zotero\\storage\\5D5GS7LI\\Wilson et al. - 2014 - Spherical and Hyperbolic Embeddings of Data.pdf;C\:\\Users\\storo\\Zotero\\storage\\FFCR45NP\\6787114.html}
}

@online{yangHypformerExploringEfficient2024,
  title = {Hypformer: {{Exploring Efficient Hyperbolic Transformer Fully}} in {{Hyperbolic Space}}},
  shorttitle = {Hypformer},
  author = {Yang, Menglin and Verma, Harshit and Zhang, Delvin Ce and Liu, Jiahong and King, Irwin and Ying, Rex},
  date = {2024-07-01},
  eprint = {2407.01290},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.01290},
  url = {http://arxiv.org/abs/2407.01290},
  urldate = {2025-04-29},
  abstract = {Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\storo\\Zotero\\storage\\V2KTIU8I\\Yang et al. - 2024 - Hypformer Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space.pdf;C\:\\Users\\storo\\Zotero\\storage\\EJWHR8RS\\2407.html}
}
