\section{Transformers}\label{sec:transformers}

\subsection{Natural language processing (NLP)}\label{subsec:nlp_word_embeddings}

Natural language processing is field of study of linguistics and machine learning that focuses on understanding the interaction between computers and human language. In particular, it is related to the task of enabling computers to understand, interpret and generate human language. NLP comprises a wide range of tasks and techniques, including: 

\begin{itemize}
    \item Text classification: Assigning predefined categories to text documents based on their content. Examples include spam detection, sentiment analysis, topic classification and determining if a sentence is grammatically correct or not.  
    \item Named entity recognition (NER): Identifying and classifying named entities in text, such as people, organizations, locations and dates.
    \item Part-of-speech tagging: Assigning grammatical categories (e.g., noun, verb, adjective) to words in a sentence.
    \item Machine translation: Automatically translating text from one language to another.
    \item Text generation: Creating coherent and contextually relevant text based on a given input or prompt.
    \item Question answering: Given a question and a context, extracting the answer to the question based on the information in the context.
\end{itemize}

\subsubsection{Word embeddings}\label{subsubsec:word_embeddings}

Many languages can be seen as a collection of sequential data where each element of the sequence is a word and they are separated by white spaces along with punctuation symbols. The first step in processing this kind of data is to find a way to convert word into a numerical representation that is suitable to be used as input to a deep neural network. It is worth mentioning that the most simple approach to this task is to use a one-hot encoding representation of the words. This means that each word is represented as a vector of zeros with a single one at the position corresponding to the index of the word in some predetermined dictionary. For example, if we have a vocabulary of 10,000 words, the word \textit{cat} would be represented as a vector of size 10,000 with a one at the index corresponding to \textit{cat} and zeros everywhere else. Of course, a realistic vocabulary or dictionary might have several hundred thousand entries, leading to very large (high dimensional) representation vectors. Also this representation does not capture any semantic information about the words. For example, the words \textit{cat} and \textit{dog} would be represented by two different vectors that are orthogonal to each other, even though they are semantically related to the word \textit{animal}. Both issues can be addressed by using \emph{word embeddings}, which are dense vector representations of words that capture their semantic meaning. Word embeddings are typically learned from large corpora of text using unsupervised learning techniques. The idea is to represent words in a continuous vector space where semantically similar words are close to each other. 

\subsubsection*{Word2vec}
The embedding process can be described as follows: We suppose we have some dictionary of words of size $m$ and we want to represent words in an embedding space of dimension $N$. Then for each one-hot encoded input vector $x_i$ we can compute the corresponding embedding vector $y_i$ as follows:

\begin{equation}
    y_i = E x_i
\end{equation}
where $E$ is an $N \times m$ matrix of learnable parameters. The matrix $E$ is usually called the \emph{embedding matrix}. The idea is to learn the parameters of the embedding matrix $E$ in such a way that the resulting embedding vectors $y_i$ capture the semantic meaning of the input words. The matrix $E$ can be learned from a corpus (a large data set) of text. There are many techniques to learn the parameters of the embedding matrix. One of the most popular techniques is referred to as \emph{word2vec} \cite{mikolovEfficientEstimationWord2013}. This is just one of many frameworks to learning word vectors. 

\begin{remark}
    Notice that since the input vector $x_i$ has a one-hot encoding representation, the output vector $y_i$ is simply the $i$-th column of the embedding matrix $E$. This means that the embedding matrix can be seen as a collection of embedding vectors, one for each word in the dictionary. 
\end{remark}

The overall architecture behind the word2vec framework consists of a simple two-layer neural network. We will first state two general considerations and later we will delve into two specific architectures used in this framework: \emph{skip-gram} and \emph{continuous bag of words} (CBOW).  

\begin{itemize}
    \item We assume a \emph{distributional hypothesis}, which simply states that words that occur in similar contexts tend to have similar meanings. This means that if two words appear in the same context (e.g., in the same sentence or paragraph), they are likely to have similar meanings. This is a very strong assumption and it is not always true, but it is a good starting point for learning word embeddings.

    \item The architecture consists of a shallow neural network with a single hidden layer. The input layer will receive one-hot encoded vectors of dimension $m$ (The length of our vocabulary). The hidden layer is a standard fully connected (Dense) layer whose weights are the word embeddings. The output layer will provide probabilities for the target words from the vocabulary. Notice that the number of units in the hidden layer is equal to the the length of the embedding vectors we want, \ie, $N$. 
    
    \begin{equation*}
    \begin{tikzpicture}
        % Input Layer
        \Vertex[x=0, y=2.8,shape=rectangle, label={$0$},fontsize=\normalsize]{}
        \Vertex[x=0, y=2.1,shape=rectangle, label={$0$},fontsize=\normalsize]{}
        \Vertex[x=0, y=1.4,shape=rectangle, label={$0$},fontsize=\normalsize]{} 
        \Vertex[x=0, y=0.7,shape=rectangle, label={$0$},fontsize=\normalsize]{}
        \Vertex[x=0, y=0,shape=rectangle, label={$1$},fontsize=\normalsize]{}
        \Vertex[x=0, y=-0.7,shape=rectangle, label={$\vdots$},fontsize=\normalsize]{}
        \Vertex[x=0, y=-1.4,shape=rectangle, label={$0$},fontsize=\normalsize]{}
        \Vertex[x=0, y=-2.1,shape=rectangle, label={$0$},fontsize=\normalsize]{}
        \Vertex[x=0, y=-2.8,shape=rectangle, label={$0$},fontsize=\normalsize]{}
        \node (input) at (1,0) {};

        % Hidden Layer
        \Vertex[x=3, y=2.5, size=1, color=orange, opacity=0.2, label={$\Sigma$},fontsize=\normalsize]{H1}
        \Vertex[x=3, y=1, size=1, color=orange, opacity=0.2, label={$\Sigma$},fontsize=\normalsize]{H2}
        \Vertex[x=3, y=-0.5, size=1, color=orange, opacity=0.2,label={$\Sigma$},fontsize=\normalsize]{H3} 
        \Text[x=3, y=-1.5,fontsize=\normalsize]{$\vdots$}
        \Vertex[x=3, y=-2.5, size=1, color=orange, opacity=0.2,label={$\Sigma$},fontsize=\normalsize]{H4}
        
        % Output Layer
        \Vertex[x=6, y=3, size=1, color=red, opacity=0.2, label={$\Sigma$}]{O1}
        \Vertex[x=6, y=1.5, size=1, color=red, opacity=0.2, label={$\Sigma$}]{O2}
        \Vertex[x=6, y=0, size=1, color=red, opacity=0.2, label={$\Sigma$}]{O3}
        \Text[x=6, y=-1.5,fontsize=\normalsize]{$\vdots$}
        \Vertex[x=6, y=-3, size=1, color=red, opacity=0.2, label={$\Sigma$}]{O4}

        % Some nodes after the output layer
        \node (T1) at (8.5,3)  {Probability};
        \node (T2) at (8.5,1.5)  {Probability};
        \node (T3) at (8.5,0)  {Probability};
        \node (T4) at (8.5,-3)  {Probability};

        % % Input to Hidden
        \foreach \i in {1,...,4} {
            \Edge[Direct,opacity=1](input)(H\i)
        }
        % Hidden to Output
        \foreach \i in {1,...,4} {
            \Edge[Direct,opacity=1](H1)(O\i)
            \Edge[Direct,opacity=0.8](H2)(O\i)
            \Edge[Direct,opacity=0.6](H3)(O\i)
            \Edge[Direct,opacity=0.4](H4)(O\i)
        }
        
        % Output arrows
        \foreach \i in {1,...,4} {
            \Edge[Direct,opacity=1](O\i)(T\i)
        }

        % Labels
        \node at (3,4.2) {\textbf{Hidden Layer}};
        \node at (6,4.2) {\textbf{Output Layer}};
        \node at (0,4.2) {\textbf{Input Layer}};
        \end{tikzpicture}  
\end{equation*}
\end{itemize}



\subsection{Encoder - Decoder Architecture}\label{subsec:encoder_decoder}

\subsubsection{Encoder}\label{subsubsec:encoder}



\subsubsection{Decoder}\label{subsubsec:decoder}

\subsection{Attention mechanism}\label{subsec:attention_mechanism}

The fundamental idea behind a transformer model is the \emph{attention} mechanism, which allows the model to focus on different parts of the input sequence when making predictions. This mechanism arose from the need to improve the performance of recurrent neural networks (RNNs) for machine translation tasks \cite{bahdanauNeuralMachineTranslation2016}. Later on, performance was improved considerably by eliminating the RNN architecture altogether and using a fully attention-based architecture, which is the basis of the transformer model \cite{vaswaniAttentionAllYou2017}. 


Let us consider the following three sentences as an example:

\begin{quote}
    \textit{I need to \textbf{run} to catch the bus!}  \\
    \textit{Paul decided to \textbf{run} for president.} \\
    \textit{We had a \textbf{run} of bad luck.} 
\end{quote}

In each case, the word \textit{run} has a different meaning depending on the context. The attention mechanism allows the model to focus on the surrounding words to determine the meaning of \textit{run} in each case. For example, in the first sentence, the model can pay more attention to the words \textit{catch} and \textit{bus}, while in the second sentence, it can focus on \textit{Paul} and \textit{president}.

\subsubsection{Processing}\label{subsubsec:processing}

The input data to a transformer is a collection of vectors $\set{\vec{x}^{(i)}}$ in $\RR^m$ where ${i=1,...,N}$. As it is usual in these notes, each element of the $i$-th vector $\vec{x}^{(i)}_j$ is called a \emph{feature} and the data vectors are called \emph{tokens}. These tokens may correspond to words withing a corpus of text, to a patch of pixels within an image, or to any other type of data sensible to be represented (embedded) as a vector. We will associate a matrix $X \in \RR^{N \times m}$ to the collection of vectors $\set{\vec{x}^{(i)}}$ as follows: 

\begin{equation}
    X = \begin{bmatrix}
       -- (\vec{x}^{(1)})^T -- \\
       -- (\vec{x}^{(2)})^T -- \\
        \vdots \\
       -- (\vec{x}^{(N)})^T --
    \end{bmatrix} 
\end{equation}   

The fundamental block of the Transformer will take the matrix $X$ as input and create a new matrix, $\widehat{X}$ of the same size: 

$$
\widetilde{X} = \textrm{TransformerLayer}(X)
$$

The idea is to create a new matrix $\widetilde{X}$ that contains the same information as $X$, but with the features of each token enhanced by the attention mechanism. We can of course stack (compose) several of these layers in sequence to create a deeper model capable of learning more complex relationships between the tokens. This single transformer layer has two stages: the one acting on columns (features) corresponding to the attention mechanism, and the one acting on rows (tokens) which corresponds to the effect of transforming the features within each token. 


\subsubsection*{Attention}

Let us denote by $\vec{y}^{(1)},...,\vec{y}^{(N)}$ the rows (tokens) of the matrix $\widetilde{X}$. Each of these tokens should live in an embedding space with a richer semantic structure than the tokens of $X$. Since each token in $X$ correspond to some data type (say words) and we want to capture some semantic relation between them, each vector $\vec{y}^{(i)}$ should depend on all the tokens from $X$, \ie, $\vec{x}^{(1)}, \vec{x}^{(2)},...,\vec{x}^{(N)}$. The simplest thing to do is to assume that each $\vec{y}^{(i)}$ depends linearly, or it is linear combination of the tokens in $X$: 

\[ \vec{y}^{(i)} = \sum_{j=1}^N a_{ij} \vec{x}^{(j)} \]

where $\alpha_{ij}$ are the coefficients of the linear combination. These coefficients will be called \emph{attention weights}. We expect these coefficients to be close to zero whenever the input tokens are not relevant to the output token. For instance, in our previous example: \emph{``I need to run to catch the bus''}, the attention weights for the output token associated with the word \textit{catch} should be high for the words \textit{catch}, \textit{to} (second), \textit{bus} and \textit{run} for we need to focus on the object of the action (the bus) and the preceding action (run) to understand the meaning of \textit{catch}. On the other hand, the attention weights should be low for the words \textit{I} and \textit{need} and the first \textit{to}.    

In the following \stnote{add specific ref label to the table} table we present some linguistic intuition about how the weights should be distributed. The first column contains the words of the sentence, the second and third columns contain notation for the input and output tokens, respectively. The fourth column contains the words that should receive low attention weights, while the fifth column contains the words that should receive high attention weights.

\begin{table}[htp] % h=here, t=top, b=bottom, p=page of floats
    \centering{
    %\caption{Intuitive Self-Attention Weights for "I need to run to catch the bus"}
    \label{tab:attention_intuition}
    \begin{tabular}{@{} l c c p{3.5cm} p{3.5cm} @{}} % Use @{} to remove padding at edges
      \toprule % Nicer top line from booktabs
      \textbf{Word} & \textbf{Input Token} & \textbf{Output Token} & \textbf{Low Attention}  & \textbf{High Attention}  \\
      \midrule % Nicer middle line from booktabs
  
      I & $x^1$ & $y^1$ & the, bus, catch & I, need, run \\
      \addlinespace % Add a bit of vertical space
  
      need & $x^2$ & $y^2$ & the, bus & need, I, to (first), run \\
      \addlinespace
  
      to & $x^3$ & $y^3$ & the, bus, I, catch & to (first), need, run \\
      \addlinespace
  
      run & $x^4$ & $y^4$ & I, need & run, to (first), to (second), catch, bus \\
      \addlinespace
  
      to & $x^5$ & $y^5$ & I, need, to (first) & to (second), run, catch, bus \\
      \addlinespace
  
      catch & $x^6$ & $y^6$ & I, need, to (first) & catch, to (second), run, the, bus \\
      \addlinespace
  
      the & $x^7$ & $y^7$ & I, need, to (first), run & the, bus, catch \\
      \addlinespace
  
      bus & $x^8$ & $y^8$ & I, need & bus, the, catch, run \\
  
      \bottomrule % Nicer bottom line from booktabs
    \end{tabular}}
\end{table}

We will then impose the following constraints on the attention weights:

\begin{itemize}
    \item The attention weights are non-negative: $a_{ij} \geq 0$, as we want to avoid situations in which one coefficient can become large and positive while another one compensated by being large and negative. This is not desirable in our case, as we want to focus on the most relevant tokens.
    \item The attention weights sum to one: $\sum_{j=1}^N a_{ij} = 1$. This is a normalization condition that ensures that if an output token pays more attention to one input token, it pays less attention to the others. 
\end{itemize}

\begin{remark}
    Notice that if we assume that the outputs are instead linear combinations of basis functions of the input tokens $\phi_1,...,\phi_N : \RR^m \to \RR$, the two conditions above ensure that the basis functions form a partition of unity. 
    \stnote{Maybe we have to add some more details here. So far it is not that important, just intuition we get from this}
\end{remark}


\subsubsection*{Self-attention}
    Let us discuss how to determine the attention weights $a_{ij}$. The idea first is to use an approach similar to the one used in problems related with information retrieval. The following image shows the basic idea used to find the attention weights: 

    \stnoteil{add image here}


    The main idea is to see each of the input vectors $\vec{x}^{(i)}$ as a \textsc{value} vector that will be used to create the output tokens. We will also use $\vec{x}^{(i)}$ as the \textsc{key} vector for the $i$th input token. Finally  we consider each $\vec{x}^{(j)}$ as \textsc{query} vector for the output $\vec{y}^{(j)}$. To achieve the constraints on the attention weights, we will use a softmax function (with no probabilistic interpretation) so that: 

    \[
    a_{ij} = \frac{\exp((\vec{x}^{(i)})^{ T} \cdot \vec{x}^{(j)})}{\sum_{k=1}^N \exp((\vec{x}^{(i)})^{T} \cdot \vec{x}^{(k)})}
    \]
    By grouping all the output tokens in a single $N \times m$ matrix $Y$ we obtain a nice matrix formula for the output tokens: 
    \begin{equation}\label{Eq.subsec.self_attention.1}
        Y = \textrm{Softmax}(X X^\top) X 
    \end{equation}
    where the softmax function applied on a $N\times N$ matrix $C = [c_{ij}]$ is a new matrix whose entries given by:
    \[ \textrm{Softmax}(C)_{ij} = \frac{\exp(c_{ij})}{\sum_{k=1}^N \exp(c_{ik})} \] 
    This process is called \emph{self-attention} because the same input tokens are used as queries, keys and values. The attention weights are computed by taking the dot product of the query vector with the key vectors, and then applying the softmax function. We will see some variations of this later on.
    
    \subsubsection*{Network parameters}
    So far, the transformation we have described to find the output tokens is fixed in the sense that there are no adjustable parameters and therefore this has no learning capacity from data. We would like to build a network that has some flexibility to choose features to focus on when determining the output tokens. For this, we can start by defining modified feature vectors via a linear transformation to the input tokens through a matrix $U$ of learnable parameters:

    \[ \widehat{X} = X U \]
    where $U$ is an $m \times m$ matrix of learnable weight parameters. Notice that this is analogous to a linear layer in a neural network. By replacing $X$ by $\widehat{X}$ in \eqref{Eq.subsec.self_attention.1} we obtain the following expression for the output tokens:

    \begin{align}\label{Eq.subsec.self_attention.2}
        Y &= \textrm{Softmax}(\widehat{X} \widehat{X}^\top) \widehat{X} \nonumber \\
        &= \textrm{Softmax}(X U U^\top X^\top) X U 
    \end{align}

    \begin{remark}
        This approach has one remarkable characteristic. The matrix $\widehat{X}\widehat{X}^\top$ is symmetric which will in turn imply a symmetric behavior in the attention mechanism. We need much more flexibility, for instance, many tasks in NLP require tokens (words) to be strongly associated with other tokens but not in the opposite sense: The word \textit{hardware} is strongly associated with the word \textit{computer} but the latter may be associated with many other words and its association with the word \textit{hardware} may not be that strong. 
    \end{remark}

    To overcome this limitation we still use the same idea but with independent learnable weight matrices for the query, key and value vectors. We will denote these matrices by $W_q$, $W_k$ and $W_v$ respectively. We then consider: 

    \begin{align*}
        Q &= X W_q  &&\dim W_q = m\times m_k \\ 
        K &= X W_k  &&\dim W_k = m\times m_k \\
        V &= X W_v &&\dim W_v = m\times m_v
    \end{align*}
    where $m_k$ and $m_v$ are the dimensions of the key and value vectors respectively. The dimensions are chosen so that we can perform dot products between the query and key vectors (a typycal choice is to take $m_k= m$). Additionally, $m_v$ will govern the dimension of the output tokens. Finally we obtain an expression for the output tokens as follows:
    \begin{equation}\label{Eq.subsec.self_attention.3}
        Y = \textrm{Softmax}(Q K^\top) V 
    \end{equation}
    
    \begin{remark}
        Recall that gradients of the softmax function become exponentially small for high magnitude inputs. To avoid these numerical issues, we can use rescale the product of the query and key vectors before applying softmax. This is done by dividing the dot product $Q K^\top$ by $\sqrt{m_k}$.  The \emph{scaled self-attention} then takes the form:
        \begin{equation}\label{Eq.subsec.self_attention.4}
            Y = \textrm{Softmax}\left(\frac{Q K^\top}{\sqrt{m_k}}\right) V
        \end{equation}
    \end{remark}
    The structure of the scaled self-attention layer is summarized in the following diagram: 

    

\subsection{Transformer models}\label{subsec:transformer_models}
