\section{Transformers}\label{sec:transformers}

\subsection{Natural language processing (NLP): word embeddings}\label{subsec:nlp_word_embeddings}


\subsection{Attention mechanism}\label{subsec:attention_mechanism}

The fundamental idea behind a transformer model is the \emph{attention} mechanism, which allows the model to focus on different parts of the input sequence when making predictions. This mechanism arose from the need to improve the performance of recurrent neural networks (RNNs) for machine translation tasks \cite{bahdanauNeuralMachineTranslation2016}. Later on, performance was improved considerably by eliminating the RNN architecture altogether and using a fully attention-based architecture, which is the basis of the transformer model \cite{vaswaniAttentionAllYou2017}. 





\subsection{Transformer models}\label{subsec:transformer_models}